

# **Advancing Large Language Models: From Text Generation to Agentic Interaction with External Systems**

## **Executive Summary**

The landscape of Large Language Models (LLMs) is undergoing a profound transformation, moving beyond their initial capabilities in natural language understanding and generation. This report examines two pivotal mechanisms driving this evolution: the development of sophisticated Q\&A systems for structured data querying and the broader integration of general tool use. These advancements fundamentally redefine the role of LLMs, positioning them not merely as passive text generators but as active, intelligent agents capable of dynamic interaction with external environments and real-world problem-solving.

The ability of an LLM system to query structured data, for instance, represents a qualitative shift from processing unstructured text. Unlike vector database searches for unstructured content, structured data interaction often involves the LLM writing and executing queries in a Domain Specific Language (DSL) like SQL.1 Similarly, the capacity for LLMs to assess a user's query, determine the utility of available tools, and then construct and execute tool-use requests, whether client-side or server-side, demonstrates a significant leap in their operational scope.2 This active engagement, encompassing decision-making, planning, and the execution of external functions, marks the emergence of LLMs as true agents.

Furthermore, this newfound agency, grounded in the ability to interact with external data and perform actions, inherently addresses a critical challenge in LLM deployment: the propensity for factual inaccuracies or "hallucinations." By directly querying structured databases for current, verified information or performing real-time web searches via server tools, LLMs can derive responses verifiable against authoritative external sources.1 This capability significantly enhances the reliability and trustworthiness of LLM outputs, making them increasingly viable for enterprise and mission-critical applications where factual accuracy is paramount.

## **Introduction: Expanding LLM Capabilities Beyond Text Generation**

### **The Paradigm Shift towards LLM Agents and External Interaction**

Initially, Large Language Models gained prominence for their remarkable proficiency in natural language understanding and generation, excelling at tasks such as summarization, translation, and creative content creation. Their utility was largely confined to processing and producing text based on their vast pre-trained knowledge. However, the true potential of LLMs is realized when they transcend these text-centric operations, evolving into active "agents." These agents are characterized by their capacity to perceive their environment through prompts and tool outputs, reason about complex tasks, and execute actions within the digital or even physical world. This evolution represents a fundamental redefinition of LLM utility, transforming them from sophisticated conversational interfaces into functional components within broader intelligent systems.

The integration of external data querying and tool execution fundamentally transforms LLMs from static knowledge repositories into dynamic problem-solvers, significantly expanding their practical applicability beyond mere conversational interfaces. While LLMs are trained on immense datasets, that knowledge is fixed at the time of training. The ability to write and execute queries against a live database or leverage a web search tool means the LLM can dynamically acquire new, real-time information and act upon it. This capability extends beyond simple fact retrieval; it enables the LLM to solve problems that demand current data and interaction with external systems, such as providing the current weather or retrieving the latest news. This shift elevates LLMs from passive responders to active participants in complex problem-solving workflows.

### **The Necessity of Tools for Real-World Utility, Accuracy, and Dynamic Data Access**

Despite their impressive knowledge and linguistic abilities, LLMs possess inherent limitations. They lack inherent real-time information access, struggle with precise calculations, and cannot directly interact with external software systems or Application Programming Interfaces (APIs). Tools serve as the crucial bridge, extending the LLM's capabilities by allowing it to delegate specific tasks to specialized external functions. This delegation enables LLMs to:

* Access up-to-date information, such as current weather conditions or the latest news, through mechanisms like web search tools.2  
* Perform precise computations or logical operations that are often difficult or unreliable for raw LLMs.  
* Interact with proprietary databases, enterprise resource planning (ERP) systems, or other external services.  
* Ground their responses in verifiable, dynamic data, thereby enhancing factual accuracy and significantly reducing the occurrence of factual errors or "hallucinations."

This evolution necessitates a re-evaluation of traditional LLM architecture, moving from monolithic models to modular, distributed systems where the LLM acts as a central orchestrator of specialized, external functions or microservices. The explicit mention of distinct, named components within frameworks, such as SQLDatabase and QuerySQLDatabaseTool in Langchain, and the clear categorization of tools into client and server types by providers like Anthropic, each with distinct execution responsibilities, strongly suggests a departure from a single, self-contained LLM.1 Instead, the LLM is integrated into a larger ecosystem of specialized modules or services. The LLM's primary role becomes one of intelligent routing and orchestration: it comprehends the user's intent, determines which external function is required, formats the request appropriately, and then integrates the response from that external function. This architectural paradigm aligns with modern software engineering principles, particularly microservices, implying that future AI systems will increasingly be built as compositions of highly specialized, interoperable components, with LLMs serving as the intelligent "glue" or "control plane" that enables these components to work together seamlessly to achieve complex goals.

## **Section 1: LLMs for Structured Data Querying (SQL Q\&A Systems)**

### **Conceptual Framework**

The interaction with structured data, typically residing in relational databases, fundamentally differs from processing unstructured text. For unstructured content, such as documents or web pages, the common approach involves generating vector embeddings and performing semantic similarity searches within a vector database. In contrast, interacting with structured data demands adherence to a Domain Specific Language (DSL), most commonly SQL. This requires the LLM not only to understand natural language intent but also to accurately map that intent to a precise, syntactically correct, and semantically meaningful SQL query that respects the underlying database schema. As noted, "Enabling a LLM system to query structured data can be qualitatively different from unstructured text data".1

The core challenge in this domain lies in the LLM's ability to bridge the gap between the ambiguity and flexibility inherent in natural language and the rigid, logical structure of SQL. This process involves sophisticated semantic parsing, accurate identification of entities, attributes, and relationships expressed within the user's query, and subsequently mapping them to the correct tables, columns, and operations (e.g., filtering, aggregation, joining) within the specific database schema.

The efficacy of an LLM-powered SQL Q\&A system is critically dependent on the LLM's ability to accurately infer and internalize the underlying database schema and semantics from natural language, representing a sophisticated form of semantic parsing and schema grounding. Generating a correct SQL query is not merely about syntax; it is about semantic accuracy relative to the database. For example, if a user asks "Show me sales data," the LLM must discern that "sales data" corresponds to a Sales table and identify relevant columns such as amount, date, or product\_id. This necessitates that the LLM possesses a robust internal representation or has access to the database schema. Components like ChatPromptTemplate are instrumental in facilitating this by injecting schema information directly into the prompt, guiding the LLM's understanding.1 This process, where abstract natural language concepts are precisely mapped to concrete database entities and relationships, is a sophisticated form of semantic grounding, which is paramount for the system's accuracy and reliability. Without effective schema grounding, the LLM might generate syntactically valid but semantically incorrect queries, leading to erroneous results.

### **Core Workflow Breakdown**

A typical LLM-powered SQL Q\&A system follows a sequence of well-defined steps to convert a natural language question into an executable query and then formulate an answer:

1. **Convert Question to SQL Query:** This initial and crucial step involves the LLM receiving a user's natural language question, such as "How many customers are in New York?". Leveraging its linguistic understanding and potentially provided database schema information, the LLM generates an executable SQL query, for instance, SELECT COUNT(\*) FROM Customers WHERE City \= 'New York';. This process transforms human intent into a machine-executable instruction. The model converts user input to a SQL query, often utilizing an LLM initialized via a chat model, such as "gemini-2.0-flash" from "google\_genai".1  
2. **Execute SQL Query:** The SQL query generated by the LLM is then transmitted to an external database connector or execution engine. This step operates independently of the LLM's core intelligence, relying on standard database interaction protocols. The database processes the query and returns the results. The system executes the query as a distinct action.1  
3. **Answer the Question:** The raw results obtained from the SQL query execution (ee.g., a numerical value, a table of records) are fed back to the LLM. The LLM then interprets these results within the context of the original user question and formulates a coherent, natural language answer that directly addresses the user's intent. The model responds to user input using the query results.1

### **Architectural Components & Frameworks**

Frameworks like Langchain provide modular components that streamline the development of SQL Q\&A systems. These components facilitate the various stages of the workflow:

* **SQLDatabase:** This component is fundamental for establishing and managing the connection to the target SQL database. It abstracts away the low-level details of database drivers and connection pooling, providing a unified interface for interaction.1  
* **QuerySQLDatabaseTool:** This tool functions as an executable unit that the LLM can invoke. It encapsulates the logic for taking a generated SQL query and executing it against the SQLDatabase instance, subsequently returning the results. This component serves as the essential bridge between the LLM's conceptual output (the SQL query) and the practical action (database execution).1  
* **ChatPromptTemplate:** This component is crucial for structuring the prompts delivered to the LLM. It ensures that the LLM receives not only the user's question but also vital contextual information, such as the database schema (including table names, column names, and relationships) and potentially few-shot examples, to guide its SQL generation and result interpretation with accuracy.1  
* **StateGraph and MemorySaver:** While not explicitly detailed for the core SQL Q\&A flow in the provided information, their inclusion in API references suggests capabilities for managing conversational state and memory.1 In more complex, multi-turn SQL Q\&A scenarios, these components would be vital for maintaining context across successive queries and enabling follow-up questions.

The system relies on a powerful LLM, such as "gemini-2.0-flash" from "google\_genai," as the core intelligence engine for natural language understanding, SQL generation, and natural language response formulation.1 Secure handling of API keys, demonstrated by the use of

getpass and os.environ for GOOGLE\_API\_KEY, is a practical necessity for authenticating and accessing commercial LLM services.1

The architectural separation of SQL query generation (performed by the LLM) from query execution (handled by an external database system) introduces a critical security and performance boundary. This necessitates robust validation, sanitization, and optimization layers to prevent SQL injection vulnerabilities or inefficient query execution. Because the LLM generates the SQL, there is an inherent risk. While LLMs are powerful, they can sometimes produce unexpected or even malicious outputs. If a user were to craft a prompt designed to trick the LLM into generating a SQL injection attack (e.g., by embedding DROP TABLE statements), and this SQL were executed directly, it would pose a severe security risk. Similarly, a poorly optimized query generated by the LLM (e.g., a full table scan on a large table when an index could be utilized) could significantly degrade database performance. This architectural delineation inherently demands the implementation of a "SQL firewall" or a validation layer situated between the LLM's output and the database. This intermediary layer would scrutinize the generated SQL for security vulnerabilities, enforce access controls, and potentially optimize queries before execution. This adds a critical layer of complexity and engineering effort beyond simply connecting the LLM to the database.

**Table 1: Key Langchain Components for SQL Q\&A Systems**

| Component Name | Primary Function | Role in SQL Q\&A Workflow | Supporting Reference |
| :---- | :---- | :---- | :---- |
| SQLDatabase | Manages database connection and interaction. | Establishes the link to the target database for query execution. | 1 |
| ChatPromptTemplate | Structures input prompts for the LLM. | Provides the LLM with user question, database schema, and context for accurate SQL generation. | 1 |
| QuerySQLDatabaseTool | Executes SQL queries against the database. | Acts as the executable function that takes LLM-generated SQL and runs it against the database. | 1 |
| StateGraph | Manages the flow and state of an agentic system. | Potentially used for multi-turn conversations and complex query sequences. | 1 |
| MemorySaver | Persists conversational history and context. | Supports maintaining context across multiple interactions in advanced Q\&A systems. | 1 |

## **Section 2: General LLM Tool Use and Agentic Workflows**

### **Tool Categorization**

The architecture of LLM tool integration can be broadly categorized based on where the tool's execution logic resides and who is responsible for its implementation. This distinction reflects varying trust boundaries and operational models, offering developers a trade-off between maximum control and reduced implementation overhead.

* **Client Tools:** These tools execute on the user's or developer's own systems. They encompass user-defined custom tools created and implemented by the client, as well as Anthropic-defined tools like computer use and text editor functionalities that require client-side implementation.2 This category offers maximum flexibility and control, allowing for seamless integration with proprietary internal systems or specific local environments. The developer retains complete control over the tool's logic, data access, and security. This is ideal for integrating with internal systems or performing actions that must remain within a controlled environment. However, this also places the responsibility for implementing, maintaining, securing, and scaling that tool squarely on the developer, implying a higher operational overhead and a greater degree of trust placed on the client's infrastructure.  
* **Server Tools:** In contrast, server tools are those whose execution logic resides and is performed on the LLM provider's servers, such as Anthropic's servers.2 A key advantage of these tools, like the web search tool, is that they must be specified in the API request but do not require implementation on the client's part, significantly reducing client-side development and operational overhead.2 Server tools abstract away the implementation and execution details from the client, reducing developer burden and operational complexity as the LLM provider manages the tool's infrastructure. However, this also implies less control and a higher degree of trust placed on the LLM provider for the tool's functionality, security, and data handling. The choice between client and server tools is a fundamental design decision, requiring developers to weigh the need for customizability and control against the desire for simplicity and reduced operational burden, based on the specific use case, security requirements, and available resources.

### **Client Tool Workflow**

The interaction model for client tools involves a multi-turn, request-response-action loop, signifying a deliberate "pause-and-resume" interaction model. This transforms the LLM API call from a single request-response into a multi-turn, asynchronous conversational loop for complex, interactive tasks.

1. **Provide Claude with Tools and a User Prompt:** The developer first defines the available client tools, providing their names, detailed descriptions (crafted from the model's perspective), and input schemas. These tool definitions are included in the API request along with the user's natural language prompt, for example, "What's the weather in San Francisco?".2  
2. **Claude Decides to Use a Tool:** Claude internally assesses the user's query and determines if any of the provided client tools are relevant and necessary to fulfill the request.2  
3. **Claude Constructs a Properly Formatted Tool Use Request:** If Claude determines a tool is needed, it generates a structured output specifying the tool to be called and the parameters to pass to it, based on its understanding of the user's query and the tool's schema.2  
4. **API Response Signals tool\_use:** Crucially, the API response from Claude will not contain a direct natural language answer but rather a stop\_reason of tool\_use. This signal explicitly indicates to the client application that Claude intends for an external tool to be executed.2 This  
   stop\_reason is not a final answer but an instruction for the client to perform an action. The LLM is effectively requesting more information or action, prompting the client to execute the specified tool and report back. This creates a conversational turn where the LLM "speaks" an action, the client "acts," and then "reports back" to the LLM. This asynchronous, multi-turn interaction interleaves the LLM's "thinking" process with external "doing."  
5. **Execute the Tool and Return Results:** Upon receiving the tool\_use signal, the client-side application is responsible for parsing Claude's tool request, executing the specified tool with the provided parameters, and then sending the tool's output (results) back to Claude in a subsequent API call. Claude then uses these results to formulate the final natural language response to the user.2 This architectural pattern enables LLMs to engage in more complex, multi-step problem-solving. It allows for human-in-the-loop interventions (where the client is a human or an intermediary system) and enables LLMs to tackle tasks that require dynamic data acquisition or interaction with systems that have latency, moving beyond simple, single-shot queries to persistent, agentic workflows.

### **Server Tool Workflow**

The workflow for server tools is significantly streamlined, as the execution responsibility lies with the LLM provider.

1. **Provide Claude with Tools and a User Prompt:** Similar to client tools, server tools, such as web search, are specified in the initial API request along with the user's query, for example, "Search for the latest news about AI.".2  
2. **Claude Executes the Server Tool:** If Claude determines a server tool is relevant, it automatically executes this tool on the provider's servers. This execution is transparent to the client application.2  
3. **Claude Uses the Server Tool Result to Formulate a Response:** The results generated by the server tool are automatically incorporated by Claude into its subsequent processing, leading directly to a natural language response without any intermediate client-side action.2

### **Best Practices for Tool Integration**

Effective tool integration relies on specific best practices that enhance the reliability and performance of LLM-powered agents:

* **Single Tool Preference:** While LLMs are capable of handling multiple tools, it is often advisable to provide a single, highly relevant tool for a given interaction.2 This approach can simplify the LLM's decision-making process and improve the consistency of its responses.  
* **tool\_choice for Explicit Control:** For scenarios requiring deterministic tool invocation, it is recommended to set tool\_choice to explicitly instruct the model to use a specific tool.2 This allows developers to override the LLM's natural decision process and force the use of a particular function. The recommendations to "provide a single tool" and explicitly "set  
  tool\_choice" suggest that current LLM tool-use mechanisms might perform optimally with explicit guidance. If LLMs were perfectly capable of autonomous tool selection and chaining, these recommendations would be less prominent. The fact that explicit guidance is advised implies that LLMs might struggle with ambiguity when presented with multiple tools, or might not always select the most optimal tool without a clear directive. This indicates that while LLMs are proficient at *using* tools, their internal "planning" and "tool orchestration" capabilities—that is, deciding *when* and *which* tool to use among many, or how to chain them—are still areas of active development and and represent an ongoing research challenge.  
* **Tool Description from Model's Perspective:** A critical aspect of effective tool integration is how tools are described to the LLM. It is emphasized that the name and description of the tool should be from the model's perspective.2 This means crafting descriptions that align with the LLM's linguistic understanding and internal reasoning patterns, enabling it to correctly identify the tool's purpose and how to interact with it. This emphasis on describing tools "from the model's perspective" highlights the critical importance of prompt engineering and clear semantic alignment between human intent, tool functionality, and the LLM's internal representation for effective and reliable tool invocation. This is not merely about writing clear documentation for human developers; it is about crafting descriptions that the LLM, based on its training data and internal semantic space, can readily comprehend and map to its reasoning process. If a tool's description uses jargon or concepts unfamiliar to the LLM, or if it is phrased in a way that does not align with how the LLM processes intent, the LLM might fail to recognize when to use the tool or how to correctly formulate its inputs. This makes prompt engineering for tool descriptions a specialized skill, requiring an understanding of how LLMs interpret language. Consequently, effective tool use is a co-design problem between the tool's functional implementation and its linguistic representation, implying that the 'interface' for LLM agents is not just code, but also carefully crafted natural language descriptions, making prompt engineering a foundational discipline in building reliable LLM-powered agents.

**Table 2: Comparative Analysis: Client Tools vs. Server Tools in LLM Workflows**

| Feature/Aspect | Client Tools | Server Tools | Supporting Reference |
| :---- | :---- | :---- | :---- |
| **Execution Location** | Your systems (client-side) | Anthropic's servers (provider-side) | 2 |
| **Implementation Responsibility** | User-defined, client-implemented | Anthropic-defined, no client implementation required | 2 |
| **Example Use Case** | Interacting with proprietary internal APIs, local file system access | Web search, general knowledge retrieval | 2 |
| **Control & Customization** | Maximum control and customizability | Limited control, standardized functionality | 2 |
| **Operational Overhead** | Higher (client responsible for hosting, maintenance, security) | Lower (provider handles infrastructure) | 2 |
| **Workflow Interaction Model** | Multi-turn, asynchronous (LLM signals tool\_use, client executes, client returns results) | Streamlined, automatic (LLM executes and integrates results directly) | 2 |
| **stop\_reason Behavior** | tool\_use signals client action required | Results automatically incorporated into response | 2 |

## **Conclusion: The Future Landscape of LLM-Powered Agents**

The analysis presented synthesizes the core arguments regarding the transformative capabilities of SQL Q\&A systems and general tool use. These advancements fundamentally elevate LLMs beyond mere conversational interfaces, positioning them as powerful, actionable agents capable of interacting dynamically with the digital and potentially physical world. This evolution enables unprecedented levels of automation for complex tasks, enhances real-time data accessibility, and paves the way for more sophisticated and intelligent AI applications across various domains.

The evolution from simple text generation to complex tool orchestration signifies a paradigm shift towards "AI as a Service," where LLMs become intelligent orchestrators of specialized microservices, rather than monolithic, self-contained AI systems. This architectural pattern mirrors a microservices approach in software development. The LLM, instead of containing all functionality internally, acts as a "gateway" or "orchestrator." It comprehends user intent and then intelligently delegates specific tasks—such as data retrieval, computation, or external API calls—to specialized, external services, which are the "tools" or database systems.1 Each tool often represents a distinct, independently developed and deployed service. The LLM's intelligence lies in understanding

*which* service is needed and *how* to interact with it, rather than performing the task itself. This implies that the future of AI system design will increasingly involve building ecosystems of specialized AI models and traditional software services, with LLMs serving as the intelligent "control plane" that binds them together. This modularity significantly enhances scalability, maintainability, and allows for the rapid integration of new capabilities without requiring extensive retraining of the core LLM.

This increasing agency and external interaction capabilities will necessitate the development of entirely new paradigms for AI safety, security, and governance, as LLMs gain the ability to perform actions with direct, real-world consequences beyond mere text generation. When an LLM can execute a SQL query, its actions extend beyond mere data retrieval to potential modification or deletion of data.1 Similarly, the use of "computer use" tools implies interaction with operating systems, files, or potentially network resources.2 If these actions are not meticulously controlled, they could lead to unintended data corruption, security breaches, or even physical harm if connected to Internet of Things (IoT) or robotic systems. The consequences transition from merely generating incorrect information to potentially performing harmful actions, significantly raising the stakes. Therefore, the traditional focus on "alignment"—ensuring LLMs generate desirable text—must expand to "agentic alignment," ensuring LLMs perform desirable

*actions*. This requires the implementation of robust access control mechanisms, comprehensive audit trails for LLM decisions, sandboxing of tool execution environments, and potentially human-in-the-loop oversight for high-impact actions. Ethical frameworks will need to evolve rapidly to address critical issues of accountability and responsibility for autonomous AI agents.

Looking forward, several key developments and implications for AI system design are anticipated:

* **More Sophisticated Multi-Tool Chaining and Autonomous Planning:** LLMs are expected to evolve to autonomously plan and execute complex sequences of tool calls, requiring less explicit prompting or tool\_choice directives. This will enhance their ability to tackle multi-step problems with greater independence.  
* **Integration with Real-World Robotics and IoT Devices:** As LLMs become more adept at tool use, their capacity to control physical systems via APIs will expand. This will lead to transformative applications in robotics, smart infrastructure, and industrial automation, bridging the gap between digital intelligence and physical action.  
* **Improved Safety, Interpretability, and Auditing Mechanisms:** As LLMs gain more agency and real-world impact, the need for robust safety protocols, transparent decision-making processes, and comprehensive auditing of their actions will become paramount. This includes developing methods to understand *why* an LLM chose a particular tool or action.  
* **Emergence of Standardized Tool APIs and Agent Frameworks:** The increasing complexity of LLM-powered systems will drive the development of standardized interfaces for tools and more mature, comprehensive frameworks for building, deploying, and managing LLM agents. This will foster interoperability and accelerate development.  
* **Ethical Considerations of Autonomous LLM Agents:** The growing autonomy of LLMs will necessitate ongoing ethical discourse and the development of regulatory frameworks to address profound issues of accountability, potential bias in actions, and the ultimate control of highly capable AI agents.

#### **Cytowane prace**

1. Build a Question/Answering system over SQL data | 🦜️ LangChain, otwierano: czerwca 16, 2025, [https://python.langchain.com/docs/tutorials/sql\_qa/](https://python.langchain.com/docs/tutorials/sql_qa/)  
2. Tool use with Claude \- Anthropic API, otwierano: czerwca 16, 2025, [https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/overview](https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/overview)